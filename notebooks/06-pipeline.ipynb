{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.19.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.19.0\n",
      "Workspace name: fin-ws-wus2\n",
      "Azure region: westus2\n",
      "Subscription id: 63a4bc7f-cd60-49a3-b139-49202d485eac\n",
      "Resource group: fin-research\n",
      "Blobstore's name: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"dadbf9da-3f3b-44a8-8097-f3512ff34da8\")\n",
    "ws = Workspace.from_config(auth=interactive_auth)\n",
    "# ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline SDK-specific imports completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core import (\n",
    "    RunConfiguration,\n",
    "    ScriptRunConfig,\n",
    "    Experiment,\n",
    "    Environment,\n",
    "    Dataset,\n",
    "    Datastore,\n",
    "    Workspace,\n",
    ")\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "gpu_cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    gpu_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "# cpu_cluster_name = \"cpu-cluster2\"\n",
    "cpu_cluster_name = \"CPU-D13V2\"\n",
    "\n",
    "try:\n",
    "    cpu_compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D11_V2', \n",
    "                                                           max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    cpu_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "input_data_dataset = Dataset.File.from_files((def_data_store, 'datasets/data_mp_1000_0.0005'), validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "data_process_conda_deps = CondaDependencies.create(pip_packages=[\"pandas\", \n",
    "                                                          \"azureml-core\", \"azureml-dataset-runtime[fuse]\", \"tqdm\", \"sklearn\"])\n",
    "cpu_env = Environment(name=\"cpu_environment\")\n",
    "cpu_env.python.conda_dependencies = data_process_conda_deps\n",
    "cpu_env.docker.enabled = True\n",
    "cpu_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_keras_env = Environment.from_conda_specification(name='tensorflow-keras-2.0-gpu', file_path='../curated_env_packages/conda_dependencies.yml')\n",
    "tf_keras_env.docker.enabled = True\n",
    "tf_keras_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04'\n",
    "# from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "# tf_keras_env.docker.base_image = DEFAULT_GPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_level = 0.0005\n",
    "vol_tick = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sl_level_param = PipelineParameter(name=\"pt_sl_level\", default_value=0.0005)\n",
    "vol_tick_param = PipelineParameter(name=\"vol_tick\", default_value=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "output_path = \"datasets/trained_models\"\n",
    "# output_data_ref = OutputFileDatasetConfig(\n",
    "#     destination=(def_data_store, output_path), name=\"model_out\"\n",
    "# )\n",
    "\n",
    "bars_dataset = Dataset.File.from_files(\n",
    "    (def_data_store, \"datasets/labels_mp_0.0005_1000/\"), validate=True\n",
    ")\n",
    "training_input_data = OutputFileDatasetConfig(name=f\"training_input_data_{pt_level}_{vol_tick}\".replace('.','_')).register_on_complete(f\"training_input_data_{pt_level}_{vol_tick}\".replace('.','_'))\n",
    "testing_input_data = OutputFileDatasetConfig(name=f\"testing_input_data_{pt_level}_{vol_tick}\".replace('.','_')).register_on_complete(f\"testing_input_data_{pt_level}_{vol_tick}\".replace('.','_'))\n",
    "output_model = OutputFileDatasetConfig(name=f\"lstm_model_{pt_level}_{vol_tick}\".replace('.','_')).register_on_complete(f\"lstm_model_{pt_level}_{vol_tick}\".replace('.','_'))\n",
    "# PipelineData(f\"training_input_data_{pt_level}_{vol_tick}\".replace('.','_'), datastore=def_blob_store).as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process/Train Pipline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--sample_days',-1,\n",
    "       '--input_data', bars_dataset.as_mount(),\n",
    "       '--output_train_data', training_input_data,\n",
    "       '--output_test_data', testing_input_data]\n",
    "script_folder = '../modules/feature_extraction/'\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = cpu_env\n",
    "\n",
    "feature_extraction_step = PythonScriptStep(\n",
    "    name = 'Feature Extraction',\n",
    "    script_name=\"run.py\", \n",
    "    arguments=args,\n",
    "    inputs=[bars_dataset.as_mount()],\n",
    "    outputs=[training_input_data,testing_input_data],\n",
    "    compute_target=cpu_compute_target, \n",
    "    source_directory=script_folder,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# args = ['--data-folder', dataset.as_mount(),\n",
    "#         '--batch-size', 64,\n",
    "#         '--first-layer-neurons', 256,\n",
    "#         '--second-layer-neurons', 128,\n",
    "#         '--learning-rate', 0.01]\n",
    "train_args = ['--sample_data_size', -1,\n",
    "       '--input_data', training_input_data,    \n",
    "       '--output_model', output_model,\n",
    "       \"--epochs\", 20]\n",
    "script_folder = '../modules/train'\n",
    "gpu_run_config = RunConfiguration()\n",
    "gpu_run_config.environment = tf_keras_env\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "    name = 'Train',\n",
    "    script_name=\"run.py\", \n",
    "    arguments=train_args,\n",
    "    inputs=[training_input_data],\n",
    "    outputs=[output_model],\n",
    "    compute_target=gpu_compute_target, \n",
    "    source_directory=script_folder,\n",
    "    runconfig=gpu_run_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "pipeline_1 = Pipeline(workspace=ws, steps=[feature_extraction_step, train_step])\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Feature Extraction [b3f56521][701b6ed5-5280-4962-9b87-b1089f09a71a], (This step will run and generate new outputs)\n",
      "Created step Train [93704eae][d6748c9f-55f8-4271-848e-74c4e443c712], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 21e27a7f-bfe5-4cbb-8ca4-1beb7a6705a5\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/fin_pipeline/runs/21e27a7f-bfe5-4cbb-8ca4-1beb7a6705a5?wsid=/subscriptions/63a4bc7f-cd60-49a3-b139-49202d485eac/resourcegroups/fin-research/workspaces/fin-ws-wus2\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run1 = Experiment(ws, 'fin_pipeline').submit(pipeline_1)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
