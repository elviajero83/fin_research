{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "built-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-salon",
   "metadata": {},
   "source": [
    "# Setup requirements\n",
    "[here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment) is the guide to the development environment setup.\n",
    "\n",
    "1. install [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) (miniconda is fine) \n",
    "2. create a new env \n",
    "\n",
    "   `conda env create -f environment.yml`\n",
    "   `conda activate aml`\n",
    "   \n",
    "3. setup jupyter\n",
    "\n",
    "   `conda install notebook ipykernel`\n",
    "   `ipython kernel install --user --name aml --display-name \"aml\"`\n",
    "   \n",
    "   \n",
    "4. install azure ml libraries \n",
    "\n",
    "   `pip install azureml-core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radio-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Workspace\n",
      "-- name: fin-ws-wus2\n",
      "-- Azure region: westus2\n",
      "-- Resource group: fin-research\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"dadbf9da-3f3b-44a8-8097-f3512ff34da8\")\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config(auth=interactive_auth)\n",
    "print('Connected to Workspace',\n",
    "  '-- name: ' + ws.name,\n",
    "  '-- Azure region: ' + ws.location,\n",
    "  '-- Resource group: ' + ws.resource_group,\n",
    "  sep = '\\n')\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-romania",
   "metadata": {},
   "source": [
    "# Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driving-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-prague",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "special-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = 'train_dataset_x'\n",
    "train_dataset_y = 'train_dataset_y'\n",
    "train_dataset_w = 'train_dataset_w'\n",
    "\n",
    "test_dataset_x = 'test_dataset_x'\n",
    "test_dataset_y = 'test_dataset_y'\n",
    "test_dataset_w = 'test_dataset_w'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collective-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '../../data/datasets/'\n",
    "ds_list = [train_dataset_x,train_dataset_y,train_dataset_w, test_dataset_x, test_dataset_y, test_dataset_w, ]\n",
    "ds_dict = {}\n",
    "for ds_name in ds_list:\n",
    "    dataset = Dataset.get_by_name(ws, name=ds_name)\n",
    "    r = dataset.download(target_path=ds_path, overwrite=True)\n",
    "    ds_dict[ds_name]=r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "radio-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset_x': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/X_train_data.npy',\n",
       " 'train_dataset_y': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/y_1_train_data.npy',\n",
       " 'train_dataset_w': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/w_train_data.npy',\n",
       " 'test_dataset_x': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/X_test_data.npy',\n",
       " 'test_dataset_y': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/y_1_test_data.npy',\n",
       " 'test_dataset_w': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fi-d14/code/Users/ma.mahmoudzadeh/data/datasets/w_test_data.npy'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-basic",
   "metadata": {},
   "source": [
    "# Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ancient-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class BindState(Enum):\n",
    "    CASH=1\n",
    "    LONG=2\n",
    "    SHORT=0\n",
    "    \n",
    "def single_unit_trade(y_pred, ret, hold_duration, bar_duration, labels=[0,1,2]):\n",
    "    ret_sum = 0\n",
    "    hold_time = 0\n",
    "    bar_separation = 0\n",
    "    number_of_trades=0\n",
    "    state=BindState.CASH\n",
    "    for i,pred in enumerate(y_pred):\n",
    "        if state==BindState.LONG:\n",
    "            bar_separation +=bar_duration[i]\n",
    "            if bar_separation>= hold_time:\n",
    "                state=BindState.CASH\n",
    "                \n",
    "        if state==BindState.CASH and pred==labels[2]:\n",
    "                number_of_trades+=1\n",
    "                state=BindState.LONG\n",
    "                hold_time = hold_duration[i]\n",
    "                ret_sum+=ret[i]\n",
    "    return ret_sum, number_of_trades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "conservative-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train_x = os.path.join('../../data/labels_with_weights/','X_train_data.npy')\n",
    "file_path_train_y = os.path.join('../../data/labels_with_weights/','y_1_train_data.npy')\n",
    "file_path_train_w = os.path.join('../../data/labels_with_weights/','w_train_data.npy')\n",
    "file_path_test_x = os.path.join('../../data/labels_with_weights/','X_test_data.npy')\n",
    "file_path_test_y = os.path.join('../../data/labels_with_weights/','y_1_test_data.npy')\n",
    "file_path_test_w = os.path.join('../../data/labels_with_weights/','w_test_data.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "characteristic-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X_train (826971, 62), y_train, (826971,), w_train (826971,), X_test (206743, 62), y_test (206743,), w_test (206743,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(file_path_train_x)\n",
    "y_train_all = np.load(file_path_train_y)\n",
    "y_train = y_train_all[:,0].reshape(len(y_train_all))\n",
    "y_train+=1\n",
    "w_train = np.load(file_path_train_w)\n",
    "w_train = w_train.reshape(len(w_train))\n",
    "\n",
    "\n",
    "X_test = np.load(file_path_test_x)\n",
    "y_test_all = np.load(file_path_test_y)\n",
    "y_test = y_test_all[:,0].reshape(len(y_test_all))\n",
    "ret_test = y_test_all[:,1].reshape(len(y_test_all))\n",
    "hold_test = y_test_all[:,2].reshape(len(y_test_all))\n",
    "y_test+=1\n",
    "w_test = np.load(file_path_test_w)\n",
    "w_test = w_test.reshape(len(w_test))\n",
    "\n",
    "print(\n",
    "    \"shapes of X_train {}, y_train, {}, w_train {}, X_test {}, y_test {}, w_test {}\".format(\n",
    "        X_train.shape, y_train.shape, w_train.shape, X_test.shape, y_test.shape, w_test.shape\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bacterial-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "equal-observer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 826961\n",
      "length of time-series i/o (826961, 10, 62) (826961,)\n",
      "dim_0 206733\n",
      "length of time-series i/o (206733, 10, 62) (206733,)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d, y_train_3d, w_train_2d = build_timeseries(X_train, y_train, steps = time_steps, weights=w_train) \n",
    "X_test_3d, y_test_3d, w_test_2d = build_timeseries(X_test, y_test, steps = time_steps, weights=w_test) \n",
    "ret_test_2d = ret_test[time_steps:]\n",
    "w_train_2d = w_train[time_steps:]\n",
    "w_test_2d = w_test[time_steps:]\n",
    "hold_test_2d = hold_test[time_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "detailed-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X_train_2d (826961, 620), y_train, (826961,), X_test_2d (206733, 620), y_test_2d (206733,)\n"
     ]
    }
   ],
   "source": [
    "X_train_2d = X_train_3d.reshape(X_train_3d.shape[0],X_train_3d.shape[1]*X_train_3d.shape[2])\n",
    "X_test_2d = X_test_3d.reshape(X_test_3d.shape[0],X_test_3d.shape[1]*X_test_3d.shape[2])\n",
    "y_train_2d = y_train_3d\n",
    "y_test_2d = y_test_3d\n",
    "\n",
    "print(\n",
    "    \"shapes of X_train_2d {}, y_train, {}, X_test_2d {}, y_test_2d {}\".format(\n",
    "        X_train_2d.shape, y_train_2d.shape, X_test_2d.shape, y_test_2d.shape\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "above-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR gain precision: 0.41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        loss       0.53      0.85      0.65    107359\n",
      "      no hit       0.18      0.30      0.23     18058\n",
      "        gain       0.41      0.02      0.04     81316\n",
      "\n",
      "    accuracy                           0.48    206733\n",
      "   macro avg       0.37      0.39      0.31    206733\n",
      "weighted avg       0.45      0.48      0.37    206733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"LR\"\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_2d, y_train_2d, sample_weight=w_train_2d)\n",
    "y_pred_flat = model.predict(X_test_2d)\n",
    "cm = confusion_matrix(y_test_2d, y_pred_flat)\n",
    "\n",
    "print(\"{} gain precision: {:.2f}\".format(name, cm[2,2]/sum(cm[:,2])))\n",
    "print(classification_report(y_test_2d, y_pred_flat, target_names=['loss','no hit','gain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "devoted-denver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1041.25, 4264)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_unit_trade(y_pred_flat, ret_test_2d, hold_test_2d, w_test_2d, labels=[0,1,2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "solved-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lightgbm.Dataset(X_train_2d, label=y_train_2d, weight = w_train_2d)\n",
    "# val_data = lightgbm.Dataset(X_val_2d, label=y_val, reference=train_data)\n",
    "test_data = lightgbm.Dataset(X_test_2d, label=y_test_2d)#, weight = w_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "hollow-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'objective': 'multiclass',\n",
    "#     'metric': 'multi_logloss,auc_mu',\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_class':3,\n",
    "    'metric_freq': 1,\n",
    "    'is_training_metric':'true',\n",
    "    'learning_rate':0.05,\n",
    "    'num_leaves': 5,\n",
    "    'num_trees':100,\n",
    "    'feature_fraction': 0.43,\n",
    "    'bagging_fraction': 0.48,\n",
    "    'bagging_freq': 4,\n",
    "    'min_data_in_leaf':50,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 1,\n",
    "    'early_stopping_round':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "reported-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[1]\ttraining's multi_logloss: 1.02084\tvalid_1's multi_logloss: 0.97141\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's multi_logloss: 1.01692\tvalid_1's multi_logloss: 0.970125\n",
      "[3]\ttraining's multi_logloss: 1.0133\tvalid_1's multi_logloss: 0.968926\n",
      "[4]\ttraining's multi_logloss: 1.00994\tvalid_1's multi_logloss: 0.967815\n",
      "[5]\ttraining's multi_logloss: 1.00688\tvalid_1's multi_logloss: 0.966767\n",
      "[6]\ttraining's multi_logloss: 1.00402\tvalid_1's multi_logloss: 0.966139\n",
      "[7]\ttraining's multi_logloss: 1.00125\tvalid_1's multi_logloss: 0.965594\n",
      "[8]\ttraining's multi_logloss: 0.998749\tvalid_1's multi_logloss: 0.964827\n",
      "[9]\ttraining's multi_logloss: 0.99641\tvalid_1's multi_logloss: 0.963975\n",
      "[10]\ttraining's multi_logloss: 0.994219\tvalid_1's multi_logloss: 0.963591\n",
      "[11]\ttraining's multi_logloss: 0.992062\tvalid_1's multi_logloss: 0.963248\n",
      "[12]\ttraining's multi_logloss: 0.990122\tvalid_1's multi_logloss: 0.962873\n",
      "[13]\ttraining's multi_logloss: 0.988188\tvalid_1's multi_logloss: 0.962676\n",
      "[14]\ttraining's multi_logloss: 0.986362\tvalid_1's multi_logloss: 0.962447\n",
      "[15]\ttraining's multi_logloss: 0.984608\tvalid_1's multi_logloss: 0.962431\n",
      "[16]\ttraining's multi_logloss: 0.982974\tvalid_1's multi_logloss: 0.962278\n",
      "[17]\ttraining's multi_logloss: 0.981401\tvalid_1's multi_logloss: 0.96258\n",
      "[18]\ttraining's multi_logloss: 0.979823\tvalid_1's multi_logloss: 0.960904\n",
      "[19]\ttraining's multi_logloss: 0.978305\tvalid_1's multi_logloss: 0.95899\n",
      "[20]\ttraining's multi_logloss: 0.976851\tvalid_1's multi_logloss: 0.957385\n",
      "[21]\ttraining's multi_logloss: 0.975391\tvalid_1's multi_logloss: 0.955686\n",
      "[22]\ttraining's multi_logloss: 0.974011\tvalid_1's multi_logloss: 0.954054\n",
      "[23]\ttraining's multi_logloss: 0.972698\tvalid_1's multi_logloss: 0.952335\n",
      "[24]\ttraining's multi_logloss: 0.971498\tvalid_1's multi_logloss: 0.950859\n",
      "[25]\ttraining's multi_logloss: 0.970301\tvalid_1's multi_logloss: 0.949352\n",
      "[26]\ttraining's multi_logloss: 0.969154\tvalid_1's multi_logloss: 0.947903\n",
      "[27]\ttraining's multi_logloss: 0.968053\tvalid_1's multi_logloss: 0.946343\n",
      "[28]\ttraining's multi_logloss: 0.966994\tvalid_1's multi_logloss: 0.944835\n",
      "[29]\ttraining's multi_logloss: 0.965994\tvalid_1's multi_logloss: 0.943653\n",
      "[30]\ttraining's multi_logloss: 0.96502\tvalid_1's multi_logloss: 0.942483\n",
      "[31]\ttraining's multi_logloss: 0.964107\tvalid_1's multi_logloss: 0.941346\n",
      "[32]\ttraining's multi_logloss: 0.963194\tvalid_1's multi_logloss: 0.940043\n",
      "[33]\ttraining's multi_logloss: 0.962326\tvalid_1's multi_logloss: 0.938815\n",
      "[34]\ttraining's multi_logloss: 0.961479\tvalid_1's multi_logloss: 0.937596\n",
      "[35]\ttraining's multi_logloss: 0.960686\tvalid_1's multi_logloss: 0.936481\n",
      "[36]\ttraining's multi_logloss: 0.959888\tvalid_1's multi_logloss: 0.935167\n",
      "[37]\ttraining's multi_logloss: 0.959101\tvalid_1's multi_logloss: 0.934148\n",
      "[38]\ttraining's multi_logloss: 0.958332\tvalid_1's multi_logloss: 0.933151\n",
      "[39]\ttraining's multi_logloss: 0.95759\tvalid_1's multi_logloss: 0.93209\n",
      "[40]\ttraining's multi_logloss: 0.956809\tvalid_1's multi_logloss: 0.9311\n",
      "[41]\ttraining's multi_logloss: 0.956065\tvalid_1's multi_logloss: 0.930068\n",
      "[42]\ttraining's multi_logloss: 0.955337\tvalid_1's multi_logloss: 0.929123\n",
      "[43]\ttraining's multi_logloss: 0.954677\tvalid_1's multi_logloss: 0.92828\n",
      "[44]\ttraining's multi_logloss: 0.954004\tvalid_1's multi_logloss: 0.92749\n",
      "[45]\ttraining's multi_logloss: 0.953342\tvalid_1's multi_logloss: 0.926491\n",
      "[46]\ttraining's multi_logloss: 0.952698\tvalid_1's multi_logloss: 0.925535\n",
      "[47]\ttraining's multi_logloss: 0.952107\tvalid_1's multi_logloss: 0.924783\n",
      "[48]\ttraining's multi_logloss: 0.951478\tvalid_1's multi_logloss: 0.924325\n",
      "[49]\ttraining's multi_logloss: 0.950879\tvalid_1's multi_logloss: 0.923562\n",
      "[50]\ttraining's multi_logloss: 0.950315\tvalid_1's multi_logloss: 0.922854\n",
      "[51]\ttraining's multi_logloss: 0.949745\tvalid_1's multi_logloss: 0.922012\n",
      "[52]\ttraining's multi_logloss: 0.94918\tvalid_1's multi_logloss: 0.921499\n",
      "[53]\ttraining's multi_logloss: 0.948585\tvalid_1's multi_logloss: 0.920667\n",
      "[54]\ttraining's multi_logloss: 0.947995\tvalid_1's multi_logloss: 0.920004\n",
      "[55]\ttraining's multi_logloss: 0.947422\tvalid_1's multi_logloss: 0.91933\n",
      "[56]\ttraining's multi_logloss: 0.94689\tvalid_1's multi_logloss: 0.918847\n",
      "[57]\ttraining's multi_logloss: 0.946386\tvalid_1's multi_logloss: 0.918319\n",
      "[58]\ttraining's multi_logloss: 0.945883\tvalid_1's multi_logloss: 0.91764\n",
      "[59]\ttraining's multi_logloss: 0.945409\tvalid_1's multi_logloss: 0.917111\n",
      "[60]\ttraining's multi_logloss: 0.944906\tvalid_1's multi_logloss: 0.916605\n",
      "[61]\ttraining's multi_logloss: 0.944433\tvalid_1's multi_logloss: 0.916091\n",
      "[62]\ttraining's multi_logloss: 0.943984\tvalid_1's multi_logloss: 0.915642\n",
      "[63]\ttraining's multi_logloss: 0.943538\tvalid_1's multi_logloss: 0.915129\n",
      "[64]\ttraining's multi_logloss: 0.943094\tvalid_1's multi_logloss: 0.914664\n",
      "[65]\ttraining's multi_logloss: 0.942648\tvalid_1's multi_logloss: 0.914219\n",
      "[66]\ttraining's multi_logloss: 0.942236\tvalid_1's multi_logloss: 0.9138\n",
      "[67]\ttraining's multi_logloss: 0.941822\tvalid_1's multi_logloss: 0.913458\n",
      "[68]\ttraining's multi_logloss: 0.941377\tvalid_1's multi_logloss: 0.912858\n",
      "[69]\ttraining's multi_logloss: 0.940989\tvalid_1's multi_logloss: 0.912503\n",
      "[70]\ttraining's multi_logloss: 0.940588\tvalid_1's multi_logloss: 0.912174\n",
      "[71]\ttraining's multi_logloss: 0.940184\tvalid_1's multi_logloss: 0.911823\n",
      "[72]\ttraining's multi_logloss: 0.939797\tvalid_1's multi_logloss: 0.911335\n",
      "[73]\ttraining's multi_logloss: 0.939443\tvalid_1's multi_logloss: 0.910946\n",
      "[74]\ttraining's multi_logloss: 0.939083\tvalid_1's multi_logloss: 0.910616\n",
      "[75]\ttraining's multi_logloss: 0.938716\tvalid_1's multi_logloss: 0.909919\n",
      "[76]\ttraining's multi_logloss: 0.93837\tvalid_1's multi_logloss: 0.909657\n",
      "[77]\ttraining's multi_logloss: 0.938004\tvalid_1's multi_logloss: 0.909424\n",
      "[78]\ttraining's multi_logloss: 0.937658\tvalid_1's multi_logloss: 0.909225\n",
      "[79]\ttraining's multi_logloss: 0.937328\tvalid_1's multi_logloss: 0.909026\n",
      "[80]\ttraining's multi_logloss: 0.936972\tvalid_1's multi_logloss: 0.908629\n",
      "[81]\ttraining's multi_logloss: 0.936654\tvalid_1's multi_logloss: 0.908344\n",
      "[82]\ttraining's multi_logloss: 0.936335\tvalid_1's multi_logloss: 0.908063\n",
      "[83]\ttraining's multi_logloss: 0.936037\tvalid_1's multi_logloss: 0.907784\n",
      "[84]\ttraining's multi_logloss: 0.935746\tvalid_1's multi_logloss: 0.907549\n",
      "[85]\ttraining's multi_logloss: 0.93541\tvalid_1's multi_logloss: 0.907093\n",
      "[86]\ttraining's multi_logloss: 0.935095\tvalid_1's multi_logloss: 0.906868\n",
      "[87]\ttraining's multi_logloss: 0.934786\tvalid_1's multi_logloss: 0.906479\n",
      "[88]\ttraining's multi_logloss: 0.934493\tvalid_1's multi_logloss: 0.906199\n",
      "[89]\ttraining's multi_logloss: 0.9342\tvalid_1's multi_logloss: 0.905929\n",
      "[90]\ttraining's multi_logloss: 0.933921\tvalid_1's multi_logloss: 0.905728\n",
      "[91]\ttraining's multi_logloss: 0.933656\tvalid_1's multi_logloss: 0.90556\n",
      "[92]\ttraining's multi_logloss: 0.933359\tvalid_1's multi_logloss: 0.905125\n",
      "[93]\ttraining's multi_logloss: 0.93309\tvalid_1's multi_logloss: 0.904913\n",
      "[94]\ttraining's multi_logloss: 0.932829\tvalid_1's multi_logloss: 0.904689\n",
      "[95]\ttraining's multi_logloss: 0.932576\tvalid_1's multi_logloss: 0.904497\n",
      "[96]\ttraining's multi_logloss: 0.932332\tvalid_1's multi_logloss: 0.904298\n",
      "[97]\ttraining's multi_logloss: 0.932059\tvalid_1's multi_logloss: 0.903939\n",
      "[98]\ttraining's multi_logloss: 0.931819\tvalid_1's multi_logloss: 0.903893\n",
      "[99]\ttraining's multi_logloss: 0.931569\tvalid_1's multi_logloss: 0.903579\n",
      "[100]\ttraining's multi_logloss: 0.931322\tvalid_1's multi_logloss: 0.903456\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's multi_logloss: 0.931322\tvalid_1's multi_logloss: 0.903456\n"
     ]
    }
   ],
   "source": [
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lightgbm.train(params=parameters,\n",
    "                     train_set = train_data,\n",
    "                     valid_sets=[train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "charming-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        loss       0.52      0.98      0.68    107359\n",
      "      no hit       0.36      0.01      0.02     18058\n",
      "        gain       0.42      0.01      0.03     81316\n",
      "\n",
      "    accuracy                           0.52    206733\n",
      "   macro avg       0.43      0.34      0.24    206733\n",
      "weighted avg       0.46      0.52      0.36    206733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_test_2d)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test_2d, y_pred_flat, target_names=['loss','no hit','gain']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "psychological-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-129.0, 1535)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_unit_trade(y_pred_flat, ret_test_2d, hold_test_2d, w_test_2d, labels=[0,1,2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "difficult-energy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.571799253626658"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test_2d."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
